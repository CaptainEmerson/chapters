#Insights trump predictions!
*Burak Turhan, University of Oulu, Finland*

*Sharing data, models, methods, insights
In this talk I’ll share our experiences in conducting a software analytics project, e. g. bug prediction. Though the project was a huge success in terms of scholarly outcomes and performance measures, we had difficulty in communicating the results to the practitioners. It turned out that our predictions were perceived as stating the obvious – even through many different modes of communication/ representation; and the most useful and insightful representation was only a visualization of the issue reports without any predictions. Hence, exploration trumps prediction in our case!

It is important to identify and act upon coordination problems within and across distributed software development teams in the context of global software development (GSD). Such problems jeopardize the quality of the system under development, and their identification is paramount to mitigate quality related risks. In this paper, we introduce a visualization tool, namely error-handling graph, that shows the interactions of development teams in terms of the reported and fixed errors during software development, and guides project managers to focus their efforts towards locating root problem causes. We present an example based on the data collected from the issue repository of an industrial project and discuss error-handling graph’s potential implications. The initial feedback from the piloting industrial team is positive, though a thorough evaluation is still needed. In practice, error graphs could be helpful in identifying the sources of problems and the opportunities for improvement, by creating awareness through data-driven insights for enhancing the way of working within and across software development teams.

Coordination of software development activities and resource management are significant challenges, especially within multi-site and large-scale project contexts [2, 6]. One important issue is the recognition and management of dependencies between the parts of software that is to be developed by different teams, and correspondingly tools, approaches and principles have been developed for the purpose, like configuration management and issue-tracking systems. It is obvious that the dependencies in the object of work will lead to communication and cooperation issues between the people involved, and this has led to several attempts to facilitate these issues by means of support tools. Probably best known among such tools is ARIADNE developed by de Souza et al. [8] where the dependencies analyzed in a software system are reflected against authorship information in configuration management repository to produce graphs that suggest social dependencies between the members of the development team. Another dependency graph tool that can be mentioned is SYSIPHUS [7].

This paper discusses about an idea of a potentially useful dependency-tracing graph tool with some similarity to the ideas behind ARIADNE. It is a spin-off of a project in an industrial setting [6] attempting to find ways to help development coordination from software analytics perspective with a focus on predictive analysis of errors. While the goal of software analytics is to propose actionable changes to the way the projects are run [1], our study revealed that the way the results are communicated to practitioners should indeed take precedence over the accuracy of the results in order to take actions [6]. The feedback from practitioners indicated that performance figures were not useful to have an impact in their daily work and pointing out error-prone sections within files was regarded as stating the obvious. Therefore, we were motivated to shift our perspective from technical details of software analytics to computer supported cooperative work (CSCW) for enabling technology transfer to industry

Consequently, we developed multiple ways of communicating our analysis to practitioners and error-handling graphs, which visualize the interactions among teams based on the errors introduced and fixed, turned out to be the most helpful representation as it helped pinpointing communication related issues within and across teams.

##Context of the Pilot Case
The data used to construct the sample error-handling graph in this paper come from the software component of a mission-critical embedded system project developed by Elektrobit Wireless Segment in Finland. The software component is implemented in C++ language with over 100.000 lines of code. The development activities are carried out in two distinct geographical locations by multiple development and testing teams, adding up to about 60 developers in total. Data collected from the issue management tool reflects the development history of the project spanning a period of two-years. In the study, a development team volunteered to pilot the developed software analytics models and modes of information representation, providing feedback through personal communication with the R&D team during project meetings [6].

##How to read the Graph?
An error-handling graph shows the interactions among software development teams in terms of errors that are reported and fixed. The errors are linked to the teams that have reported or fixed the error. Figure 1 shows a sample graph based on the pilot project data (please see next section for details on the pilot). 
There are two types of nodes in Figure 1. The nodes with text labels correspond to the teams involved in the project. Text labels start either with the prefix DevTeam or TestTeam, meaning that the corresponding team is a development or testing team, respectively. The nodes between the text labeled nodes represents errors extracted from the issue management tool, and they are color coded by the level of testing at which they are detected. A green edge from a team node to an error node means that the corresponding team has fixed the reported error. On the other hand, a red edge means that the corresponding team has reported that error. Finally, blue lines represent the errors that are reported and fixed by the same team [6].

In an ideal scenario, one would expect to see patterns in an error-handling graph, where all errors are: (a) either reported by a testing team and fixed by the responsible development team; (b) or reported and fixed by the same development team. Moreover, an ideal scenario would assume that there would be no issues originating from the interactions among the teams, i.e. an anti-pattern. Such errors are likely to have been missed in team level testing, propagated to higher levels causing re-work, and should have been detected earlier within the team.


##Interpretation and Discussion
Analysis of Figure 1 in terms of (anti-) patterns reveals the following examples:
*Pattern (a): It can be seen that the testing team named ‘Aarne’ is reporting most of the errors, e.g. the density of the red edges are very high around that team’s node. 
*Pattern (b): Development teams named ‘Oili’ and ‘Niilo’ are following the good practice of transparency by sharing the errors discovered and fixed within the team with the rest of the project teams. 
*Anti-pattern: There exists a cluster of errors between the development teams ‘Terttu’ and ‘Kristiina’. This indicates that the development works of the teams are most likely to be dependent on each other and errors propagate out of either team’s internal quality assurance process, impacting the work of the other.

Based on the initial findings listed above, the following actionable insights can be taken into account:
*Depending on the amount of errors detected by the testing teams in each testing level, an analysis on the nature of errors can be conducted to understand which level of testing needs to be improved (checking color codes in the graph) in order to identify the errors earlier in the process. 
*All development teams can be encouraged to report the errors found within their internal processes in the global issue repository in order to improve the transparency of the development process. This would enable the collaborating teams to share insights about the problems encountered in the shared part of the code. 
*Internal quality processes of the teams can be improved, especially when errors are propagating across development teams. This would save time and effort spent on re-work activities. In addition, the communication and coordination practices between such teams can be improved in order to reduce the number of errors reported across teams.

To elaborate more on the last item: there are two potential reasons for the conflict between ‘Terttu’ and ‘Kristiina’ teams. One reason could be that the teams’ way of working may be lacking quality gates for checking how their changes affect the work of others. Alternatively, there are latent dependencies between the parts the teams are concurrently working on. Such dependencies may have not been noticed and flagged when the division of labor between the teams is planned. So, the error-handling graph can in principle indicate problems in two levels: programming and testing practices/tools inside the teams, and in planning and dependency identification practices/tools. Further, the nature of those latent dependencies may not necessarily be code-based. In other words, code-based dependencies may just be a symptom of dependencies not detected at higher levels of abstraction, e.g. conflicting requirements from different stakeholders that are missed, or changes in requirements that were not communicated to the both teams properly. In any case, the root cause is a communication/ coordination related issue that can be further worked on, yet can easily be identified by a visual inspection of the error-handling graph.

##Summary
In this paper, we introduced error-handling graphs, which are potentially helpful in visually identifying error leakage across teams, as well as the need for better interaction/ communication across teams that introduce and fix those errors. An error-handling graph shows the communication channels through the error documentation process. Accordingly, it is expected to help in identifying and handling the bottlenecks of project communication with respect to quality in terms of errors. 
In practice, error-handling graphs could be helpful CSCW tools in identifying the sources of problems and the opportunities for improvement, by creating awareness [3, 4] through data-driven insights for enhancing the way of working within and across software development teams. We have demonstrated our arguments with a pilot study conducted within a multinational organization in Finland, where “the global is in the local” [9], since the development activities are: (i) distributed in two geographical sites (ii) and relevant from GSD perspective as they depend on “... coordination between local teams and their local practices” [9].
A formal evaluation of the tool is still pending and our future work will focus on contacting the teams of the pilot project, in which (anti-) patterns are observed, in order to gain more insights about the arguments discussed in this paper. Specifically, communication issues will be investigated by focusing on the errors between the teams, as they are likely be the key errors that slow down the development process by introducing re-work. Nevertheless, the initial feedback for the tool has been positive by the piloting team. Team members stated that the error-handling graph gives an interesting point of view into the project and holds a lot of implicit information. 


##References
1.	Menzies, T. and Zimmermann, T. Software Analytics: So What? IEEE Software, vol. 30/4, pp. 31-37, 2013.
2.	Smite, D. and Turhan, B. Validity of research on large-scale agile projects. Workshop on Research Challenges in Large-Scale Agile Development (XP 2013 Workshop), 2013
3.	Souza, C. D. and Redmiles, D. The awareness network: To whom should I display my action? And, whose actions should I monitor. In Proceedings of the European Conference on Computer Supported Cooperative Work (ECSCW) (Limerick, Ireland, 2007). Kluwer Academic.
4.	Gutwin, C., Penner, R. and Schneider, K. Group awareness in distributed software development. ACM, 2004. 
5.	Halverson, C., Ellis, J., Danis, C. and Kellogg, W. Designing task visualizations to support the coordination of work in software development. In Proceedings of the Computer supported Cooperative work (CSCW) (New York, USA, 2006). ACM.
6.	Taipale, T., Turhan, B. and Qvist, M. Constructing Defect Predictors and Communicating the Outcomes to Practitioners. In Proceedings of the 7th International Symposium on Empirical Software Engineering and Measurement (ESEM 2013 Industry Track), 2013.
7.	Bruegge, B., Dutoit, A., Wolf, T. (2006) Sysiphus: Enabling informal collaboration in global software development. Proc. International Conference on Global Software Engineering (ICGSE’06), pp. 139-148.
8.	De Souza, C., Quirk, S., Trainer, E., Redmiles, D. (2007) Supporting collaborative software development through the visualization of socio-technical dependencies. In Proceedings of the 12th International ACM Conference on Supporting Groupwork (GROUP’07), pp. 147-156. 
9.	Avram, G., Bannon, L., Bowers, J., Sheehan, A. and Sullivan, D.K. Bridging, Patching and Keeping the Work Flowing: Defect Resolution in Distributed Software Development. Comput. Supported Coop. Work 18, 5-6 (2009), 477-507.
